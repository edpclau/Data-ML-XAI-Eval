{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROAD Development Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Eddie Perez Claudio\n",
    "# The goal of this script is to evaluate the performance of XAI methods on the simulated datasets using the ROAD method.\n",
    "\n",
    "## Import Libraries ##\n",
    "\n",
    "#XAI\n",
    "import shap\n",
    "\n",
    "#Imputation\n",
    "import miceforest as mf\n",
    "\n",
    "# Data wrangling\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "## Libraries ##\n",
    "#General\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "#Model Evalutation\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, average_precision_score, recall_score\n",
    "import shap\n",
    "import shap.benchmark\n",
    "\n",
    "# multiprocessing\n",
    "from multiprocess.pool import Pool\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Benchmark (Remove and Debias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove and Retrain ####\n",
    "# This is a version of ROAD (as described in Rong et al 2022) \n",
    "# which works with any scikit-learn model and tabular data (pandas dataframes)\n",
    "#### Code ####\n",
    "\n",
    "## Explanation Function ##\n",
    "# Trains a model, builds an explanation with KernelShap, and then\n",
    "# outputs the ranking of each feature in descending order.\n",
    "# Arguments:\n",
    "# clf : A trained ML model with a .predict method\n",
    "# X: Training data as pandas dataframe\n",
    "# x:  Test data as pandas dataframe\n",
    "# explainer: any explainer that follows the shap api format \n",
    "def explain(clf, X, x, explainer = shap.explainers.Permutation):\n",
    "    X = X.to_numpy()\n",
    "    x = x.to_numpy()\n",
    "\n",
    "    if explainer == shap.KernelExplainer:\n",
    "        try:\n",
    "                    # #Build Explanation\n",
    "            explanation = explainer(clf.predict_proba, shap.kmeans(X, 4))\n",
    "            shap_values = np.abs(explanation.shap_values(x)[1]).mean(0)\n",
    "            return shap_values\n",
    "\n",
    "        except:\n",
    "        # #Build Explanation\n",
    "            explanation = explainer(clf.predict, shap.kmeans(X, 4))\n",
    "            shap_values = np.abs(explanation.shap_values(x)).mean(0)\n",
    "            return shap_values\n",
    "   \n",
    "    elif explainer == shap.explainers.Tree:\n",
    "        # #Build Explanation\n",
    "        explanation = explainer(clf, X)\n",
    "        shap_values = explanation(x).values\n",
    "        if len(shap_values.shape) == 2:\n",
    "            shap_values = np.abs(shap_values).mean(0)\n",
    "        else:\n",
    "            shap_values = np.abs(shap_values[...,1]).mean(0)\n",
    "        return shap_values\n",
    "    \n",
    "    elif explainer == shap.explainers.Linear:\n",
    "        # #Build Explanation\n",
    "        explanation = explainer(clf, X)\n",
    "        shap_values = np.abs(explanation(x).values).mean(0)\n",
    "        return shap_values\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "                    # #Build Explanation\n",
    "            explanation = explainer(clf.predict_proba, X)\n",
    "            shap_values = np.abs(explanation(x).values[...,1]).mean(0)\n",
    "            return shap_values\n",
    "        \n",
    "        except:\n",
    "        # #Build Explanation\n",
    "            explanation = explainer(clf.predict, X)\n",
    "            shap_values = np.abs(explanation(x).values).mean(0)\n",
    "            return shap_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ranking Function ##\n",
    "# Outputs the ranking of each feature in descending order.\n",
    "# Arguments:\n",
    "# shap_values: the output of any shap explainer\n",
    "def ranker(shap_values):\n",
    "    values = copy.deepcopy(shap_values)     \n",
    "    #Get ranks\n",
    "    ranks = np.argsort(values)\n",
    "    return ranks\n",
    "\n",
    "\n",
    "## Metrics function ##\n",
    "# Utility function which outputs a series of metrics to evaluate\n",
    "# Currently gets accuracy, balanced_accuracy, and f-score\n",
    "\n",
    "#Arguments:\n",
    "#clf: A trained ML model with a predict method\n",
    "#x: a pd.DataFrame of test data\n",
    "#y: Target of the test data\n",
    "def metrics(clf, x, y):   \n",
    "    #Predict\n",
    "    # print(len(x))\n",
    "    yhat = clf.predict(x.to_numpy())\n",
    "    try:\n",
    "        yscore = clf.predict_proba(x.to_numpy())[:, 1]\n",
    "    except:\n",
    "        yscore = clf.decision_function(x.to_numpy())\n",
    "    #Get metrics\n",
    "    accu = accuracy_score(y, yhat)\n",
    "    accu_balanced = balanced_accuracy_score(y, yhat)\n",
    "    f1 = f1_score(y, yhat)\n",
    "    auroc = roc_auc_score(y, yscore)\n",
    "    auprc = average_precision_score(y, yscore)\n",
    "    recall = recall_score(y, yhat)\n",
    "        \n",
    "    return np.array([[accu], [accu_balanced], [f1], [auroc], [auprc], [recall]])\n",
    "\n",
    "\n",
    "\n",
    "## Imputation Function assistant ##\n",
    "\n",
    "def impute(i,k, rankings, x_test):\n",
    "       #Add NA values\n",
    "        x_test = mf.ampute_data(x_test, variables= x_test.columns[rankings[i:k]].to_list(), perc = 0.8, random_state=42)\n",
    "        #impute\n",
    "        variables = x_test.columns.to_list()\n",
    "        index = rankings.copy()[i:k].tolist().pop()\n",
    "        kds = mf.ImputationKernel(\n",
    "        x_test,\n",
    "        variable_schema={variables.pop(index) : variables},\n",
    "        save_all_iterations=True,\n",
    "        random_state=1991\n",
    "        )\n",
    "        # Run the MICE algorithm for 2 iterations\n",
    "        kds.mice(2)\n",
    "        # Return the completed dataset.\n",
    "        x_test = kds.complete_data()\n",
    "\n",
    "        return x_test\n",
    "\n",
    "\n",
    "## Mask Features ## \n",
    "#These functions mask data using imputation and retrain the ML model\n",
    "#They mask from the top %, bottom %, or random.\n",
    "\n",
    "# Arguments:\n",
    "# t: percentage or number of features to be masked in each iteration\n",
    "# rankings: a list of rankings (descending rankings) to guide the removal\n",
    "# X, x: dataframes of the data from which we will remove the features. Training set and testing set, respectively\n",
    "# Y, y: Targets for the training and testing sets, respectively\n",
    "#clf: Model to retrain\n",
    "# base: metrics of the full model. Used to compare the performance of the masked model\n",
    "# direction: direction of masking. Can be 'top', 'bottom', or 'random'\n",
    "# seed: seed for random number generator\n",
    "\n",
    "#Mask\n",
    "def mask(t, rankings, X, Y, x, y, clf, base = np.empty((3,1)), direction = 'top'):\n",
    "    #Make copies of our data to modify\n",
    "    X_train = copy.deepcopy(X)\n",
    "    x_test = copy.deepcopy(x)\n",
    "    results = copy.deepcopy(base)\n",
    "\n",
    "    ## Directional Masking ##\n",
    "    #Top\n",
    "    if direction == 'top':\n",
    "        #Set masking schedule and iterator\n",
    "        if type(t) != int:\n",
    "            j = int(np.round(len(rankings)*t))\n",
    "            i = len(rankings) - j\n",
    "            k = len(rankings)\n",
    "        else: \n",
    "            j = t\n",
    "            i = len(rankings) - t\n",
    "            k = len(rankings)\n",
    "\n",
    "        #Impute and Predict\n",
    "        while k >= j:\n",
    "            #Impute\n",
    "            x_test = impute(i,k, rankings, x_test)\n",
    "            #Predict\n",
    "            results =  np.hstack((results, metrics(clf, x_test, y)))\n",
    "            \n",
    "            #Move iterator forward\n",
    "            i -= j\n",
    "            k -= j\n",
    "\n",
    "        return results \n",
    "        \n",
    "    #Bottom\n",
    "    elif direction == 'bottom':   \n",
    "        #Set masking schedule\n",
    "        if type(t) != int:\n",
    "            j = int(np.round(len(rankings)*t))\n",
    "            i = 0\n",
    "            k = j\n",
    "        else:\n",
    "            j = t\n",
    "            i = 0\n",
    "            k = j\n",
    "\n",
    "        #Impute and Predict\n",
    "        while k <= len(rankings):\n",
    "            #Impute\n",
    "            x_test = impute(i,k, rankings, x_test)\n",
    "            #Predict\n",
    "            results =  np.hstack((results, metrics(clf, x_test, y)))\n",
    "            \n",
    "            #Move iterator forward\n",
    "            i += j\n",
    "            k += j\n",
    "        return results\n",
    "    \n",
    "    #Random\n",
    "    elif direction == 'random':\n",
    "        np.random.seed(42)\n",
    "        random_choices = np.random.permutation(rankings)\n",
    "        #Set masking schedule\n",
    "        if type(t) != int:\n",
    "            j = int(np.round(len(rankings)*t))\n",
    "            i = 0\n",
    "            k = j\n",
    "        else:\n",
    "            j = t\n",
    "            i = 0\n",
    "            k = j\n",
    "\n",
    "        #Impute and Predict\n",
    "        while k <= len(rankings):\n",
    "            #Impute\n",
    "            x_test = impute(i,k, random_choices, x_test)\n",
    "            #Predict\n",
    "            results =  np.hstack((results, metrics(clf, x_test, y)))\n",
    "            \n",
    "            #Move iterator forward\n",
    "            i += j\n",
    "            k += j\n",
    "        return results\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "## ROAD ##\n",
    "# The main function of the library.\n",
    "#Wraps all other functions in a nice pipeline which is easy to use.\n",
    "#Accepts any scikit-learn model. It was built and tested using a binary target.\n",
    "\n",
    "# Arguments:\n",
    "# model : the model to be re-trained\n",
    "# t: percentage of features to be removed in each iteration\n",
    "# X: Training data as pandas dataframe\n",
    "# Y: Target values for training (This was build using a binary target)\n",
    "# x:  Test data as pandas dataframe\n",
    "# y: Target values for testing\n",
    "# explainer: any explainer which built with the shap api\n",
    "# repeats: how many times to explain and do the whole retraining\n",
    "\n",
    "#outputs accuracy, balanced_accuracy, f1_score, and ranks for each iteration. \n",
    "def road(X, Y, x, y, model, explainer = None, t = 0.10, shap_values = None, repeats = 10):\n",
    "    #Initialize variables\n",
    "    base = metrics(model, x, y)\n",
    "    if explainer != None:\n",
    "        values = explain(model, X, x, explainer)\n",
    "        ranks = ranker(values)\n",
    "       \n",
    "    elif shap_values != None:\n",
    "        values = shap_values\n",
    "        ranks = ranker(values)\n",
    "    else:\n",
    "        print('Must supply either an explainer or shap_values')\n",
    "        return\n",
    "    \n",
    "    top = mask(t, ranks, X, Y, x, y, model, base, direction='top')\n",
    "   \n",
    "    bottom = mask(t, ranks, X, Y, x, y, model, base, direction='bottom')\n",
    "  \n",
    "    random = mask(t, ranks, X, Y, x, y, model, base, direction='random')\n",
    "\n",
    "    \n",
    "\n",
    "    #Set progress bar\n",
    "    # Repeat x times\n",
    "    for i in range(repeats-1):\n",
    "                #Initialize\n",
    "        if explainer != None:\n",
    "            iter_values = explain(model, X, x, explainer)\n",
    "            ranks = ranker(iter_values)\n",
    "            values = np.dstack((values, iter_values))\n",
    "        elif shap_values != None:\n",
    "            ranks = ranker(values)\n",
    "        else:\n",
    "            print('Must supply either an explainer or shap_values')\n",
    "            return\n",
    "        top = np.dstack((top, mask(t, ranks, X, Y, x, y, model, base, direction='top')))\n",
    "        bottom = np.dstack((bottom, mask(t, ranks, X, Y, x, y, model, base, direction='bottom')))\n",
    "        random = np.dstack((random, mask(t, ranks, X, Y, x, y, model, base, direction='random')))\n",
    "    return [top, bottom, random, values]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faux Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a faux model for the data\n",
    "## This will only work for ShapSampling and ShapKernel, or ShapPermutation\n",
    "\n",
    "class faux_model:\n",
    "  def __init__(self):\n",
    "      self.name = 'FauxModel'\n",
    "\n",
    "  @staticmethod\n",
    "  def predict(x):\n",
    "    if str(type(x)).__contains__('pandas'):\n",
    "      x = x.to_numpy()\n",
    "    \n",
    "    return x[:,0:3].prod(axis = 1)\n",
    "  \n",
    "  def predict_proba(self, x):\n",
    "    yhat = self.predict(x)\n",
    "    return np.array([np.where(yhat == 0, 1, 0),yhat]).T\n",
    "\n",
    "\n",
    "# y = data['3_vars_corr_2HC_n10000.csv']['y_test']\n",
    "# x = data['3_vars_corr_2HC_n10000.csv']['X_test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_vars_corr_0HC_n25000.csv', '0_vars_corr_0HC_n10000_skew_0.5194.csv', '0_vars_corr_0HC_n100.csv', '0_vars_corr_0HC_n10000_skew_0.7354.csv', '3_vars_corr_1HC_n10000.csv', '0_vars_corr_0HC_n10000_skew_0.8612.csv', '0_vars_corr_0HC_n15000.csv', '2_vars_corr_1HC_n10000.csv', '0_vars_corr_0HC_n10000_skew_0.9694.csv', '0_vars_corr_0HC_n10000_skew_0.9699.csv', '0_vars_corr_0HC_n10000_skew_0.7298.csv', '0_vars_corr_0HC_n10000_skew_0.6145.csv', '2_vars_corr_2HC_n10000.csv', '0_vars_corr_0HC_n10000.csv', '0_vars_corr_0HC_n10000_skew_0.8608.csv', '0_vars_corr_0HC_n5000.csv', '0_vars_corr_0HC_n10000_skew_0.508.csv', '0_vars_corr_0HC_n10000_skew_0.6222.csv', '3_vars_corr_2HC_n10000B.csv', '0_vars_corr_0HC_n1000.csv', '0_vars_corr_0HC_n30000.csv', '3_vars_corr_2HC_n10000.csv', '0_vars_corr_0HC_n20000.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built\n"
     ]
    }
   ],
   "source": [
    "### Import Data ###\n",
    "# Simulated Datasets\n",
    "\n",
    "# Path to Simulated Datasets\n",
    "\n",
    "dir_path = '/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Synthetic Data/Data Files'\n",
    "\n",
    "# Lists to store files name\n",
    "res_ = []\n",
    "res = []\n",
    "for (dir_path, dir_names, file_names) in os.walk(dir_path):\n",
    "    res_.extend(file_names)\n",
    "for file in res_:\n",
    "    if file not in ['GroundTruth.csv', '.Rhistory', '.DS_Store']:\n",
    "        res.append(file)\n",
    "\n",
    "print(res)\n",
    "\n",
    "data = {}\n",
    "for file in res:\n",
    "    data_path = f'{dir_path}/{file}'\n",
    "    df = pd.read_csv(data_path)\n",
    "    train, test = train_test_split(df, test_size=0.25, random_state=42)\n",
    "    data[file] = {\n",
    "        'X_train': train.drop('Target', axis=1),\n",
    "        'y_train': train.Target,\n",
    "        'X_test': test.drop('Target', axis=1),\n",
    "        'y_test': test.Target\n",
    "    }\n",
    "\n",
    "\n",
    "### Import Models ###\n",
    "# Import Models\n",
    "names = ['PassiveAgressive', 'SGDClassifier', 'RandomForest', 'Perceptron', 'RidgeClassifier', 'LogisticRegression', 'DecisionTree', 'XGBoost', 'GaussianNB', 'FauxModel']\n",
    "file_models = {}\n",
    "for file in res:\n",
    "    models = {}\n",
    "    for model in names:\n",
    "        if model == 'FauxModel':\n",
    "            models[model] = faux_model()\n",
    "        else:\n",
    "            models[model] = joblib.load(\n",
    "                f'/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Model Training/Models/{file}_{model}.joblib')\n",
    "    file_models[file] = models\n",
    "\n",
    "### Model types ###\n",
    "# Linear Models\n",
    "linear = ['PassiveAgressive', 'SGDClassifier', 'Perceptron', 'RidgeClassifier', 'LogisticRegression']\n",
    "# Tree Models\n",
    "tree = ['RandomForest', 'DecisionTree', 'XGBoost']\n",
    "# Other\n",
    "other = ['MultinomialNB', 'GaussianNB', 'FauxModel']\n",
    "\n",
    "### Set Explainers ###\n",
    "# Explainers of Interest\n",
    "xai = {\n",
    "    'ShapLinear': shap.explainers.Linear,\n",
    "    'ShapTree': shap.explainers.Tree,\n",
    "    'ShapPermutation': shap.explainers.Permutation,\n",
    "    'ShapSampling': shap.explainers.Sampling,\n",
    "    'ShapExact' : shap.explainers.Exact,\n",
    "    'ShapKernel': shap.explainers._kernel.Kernel\n",
    "}\n",
    "\n",
    "### Define Parallel Function ###\n",
    "def parallel_road(file, model_name, exp):\n",
    "\n",
    "    #Check if exists\n",
    "    if os.path.isfile(f'/content/drive/MyDrive/XAI method performacne when Explainaing the PORT Dataset/Results/Evaluations/ROAD/SimTrack/Sampling Redo/{file}_{model_name}_{exp}.joblib'):\n",
    "      return\n",
    "\n",
    "    df = data[file]\n",
    "    models = file_models[file]\n",
    "    model = models[model_name]\n",
    "    save_path = '/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Model Explanation/ROAD Output'\n",
    "    # res_ = {}\n",
    "    # evaluate linear exps\n",
    "    if (model_name in linear) and (exp in ['ShapSampling', 'ShapLinear', 'ShapPermutation']):\n",
    "            res_ = road(df['X_train'], df['y_train'], df['X_test'],\n",
    "                                  df['y_test'], model=model, t=1,  explainer=xai[exp])\n",
    "            joblib.dump(res_, f'{save_path}/{file}_{model_name}_{exp}.joblib', compress=3)\n",
    "\n",
    "    # eval tree exps\n",
    "    elif (model_name in tree) and (exp in ['ShapSampling', 'ShapTree', 'ShapPermutation']):\n",
    "            res_ = road(df['X_train'], df['y_train'], df['X_test'],\n",
    "                                  df['y_test'], model=model, t=1,  explainer=xai[exp])\n",
    "            joblib.dump(res_, f'{save_path}/{file}_{model_name}_{exp}.joblib', compress=3)\n",
    "\n",
    "    # ev al agnostic exps\n",
    "    elif (exp in ['ShapSampling', 'ShapPermutation', 'ShapExact', 'ShapKernel']):\n",
    "            res_ = road(df['X_train'], df['y_train'], df['X_test'],\n",
    "                                  df['y_test'], model=model, t=1,  explainer=xai[exp])\n",
    "            joblib.dump(res_, f'{save_path}/{file}_{model_name}_{exp}.joblib', compress=3)\n",
    "    \n",
    "    print(f'{file}_{model_name}_{exp} done')\n",
    "    #joblib.dump(res_, f'/content/drive/MyDrive/XAI method performacne when Explainaing the PORT Dataset/Results/Evaluations/ROAD/SimOptim/{file}_{model_name}_optimized.joblib', compress=3)\n",
    "    return\n",
    "\n",
    "print('built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    }
   ],
   "source": [
    "ml = ['PassiveAgressive', 'SGDClassifier', 'RandomForest', 'Perceptron', 'RidgeClassifier', 'LogisticRegression', 'DecisionTree', 'XGBoost', 'GaussianNB', 'FauxModel']\n",
    "exp = list(xai.keys())\n",
    "\n",
    "print(len(exp)*len(res_)*len(ml))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_vars_corr_0HC_n25000.csv_PassiveAgressive_ShapTree done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 18750 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_vars_corr_0HC_n25000.csv_SGDClassifier_ShapLinear done\n",
      "0_vars_corr_0HC_n25000.csv_SGDClassifier_ShapTree done\n",
      "0_vars_corr_0HC_n25000.csv_PassiveAgressive_ShapLinear done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5b2c5f54b7426c92b4129786d8e899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_vars_corr_0HC_n25000.csv_PassiveAgressive_ShapExact done\n",
      "0_vars_corr_0HC_n25000.csv_PassiveAgressive_ShapPermutation done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 18750 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_vars_corr_0HC_n25000.csv_RandomForest_ShapLinear done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|==================  | 10939/12500 [00:12<00:01]       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Model Explanation/ROAD_dev.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability%20Method%20Comparison/Data-ML-XAI-Eval/Model%20Explanation/ROAD_dev.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pool \u001b[39m=\u001b[39m Pool(\u001b[39m5\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability%20Method%20Comparison/Data-ML-XAI-Eval/Model%20Explanation/ROAD_dev.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mstarmap_async(parallel_road, itertools\u001b[39m.\u001b[39mproduct(res_, ml, exp), chunksize\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability%20Method%20Comparison/Data-ML-XAI-Eval/Model%20Explanation/ROAD_dev.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m results\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/python_env/lib/python3.8/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python_env/lib/python3.8/site-packages/multiprocess/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/python_env/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/python_env/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|=================== | 11887/12500 [00:13<00:00]       "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Paralelization ###\n",
    "if __name__ == '__main__':\n",
    "  pool = Pool(5)\n",
    "  results = pool.starmap_async(parallel_road, itertools.product(res_, ml, exp), chunksize=1)\n",
    "  results.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d4e42ab716de065dabed38619282a5a545b37a73470e3b37377cd94259c7c27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
