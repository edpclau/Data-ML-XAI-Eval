{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle the XAI Evaluation Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling\n",
    "from imp import reload\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "# Make sure joblib version is 1.2.0\n",
    "import joblib\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "# import scipy.signal\n",
    "\n",
    "\n",
    "# #XAI Eval\n",
    "# import ROAD\n",
    "# import shap\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import DataCleaningUtils\n",
    "reload(DataCleaningUtils)\n",
    "from DataCleaningUtils import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ROAD evaluation results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths of files to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_vars_corr_0HC_n100.csv' '0_vars_corr_0HC_n1000.csv'\n",
      " '0_vars_corr_0HC_n200.csv' '0_vars_corr_0HC_n300.csv'\n",
      " '0_vars_corr_0HC_n400.csv' '0_vars_corr_0HC_n500.csv'\n",
      " '0_vars_corr_0HC_n600.csv' '0_vars_corr_0HC_n700.csv'\n",
      " '0_vars_corr_0HC_n800.csv' '0_vars_corr_0HC_n900.csv'\n",
      " '1_vars_corr_1HC_n100.csv' '1_vars_corr_1HC_n1000.csv'\n",
      " '1_vars_corr_1HC_n300.csv' '1_vars_corr_1HC_n500.csv'\n",
      " '1_vars_corr_1HC_n700.csv' '1_vars_corr_1HC_n900.csv'\n",
      " '2_vars_corr_1HC_n100.csv' '2_vars_corr_1HC_n1000.csv'\n",
      " '2_vars_corr_1HC_n300.csv' '2_vars_corr_1HC_n500.csv'\n",
      " '2_vars_corr_1HC_n700.csv' '2_vars_corr_1HC_n900.csv'\n",
      " '2_vars_corr_2HC_n100.csv' '2_vars_corr_2HC_n1000.csv'\n",
      " '2_vars_corr_2HC_n300.csv' '2_vars_corr_2HC_n500.csv'\n",
      " '2_vars_corr_2HC_n700.csv' '2_vars_corr_2HC_n900.csv'\n",
      " '3_vars_corr_1HC_n100.csv' '3_vars_corr_1HC_n1000.csv'\n",
      " '3_vars_corr_1HC_n300.csv' '3_vars_corr_1HC_n500.csv'\n",
      " '3_vars_corr_1HC_n700.csv' '3_vars_corr_1HC_n900.csv'\n",
      " '3_vars_corr_2HC_n100.csv' '3_vars_corr_2HC_n1000.csv'\n",
      " '3_vars_corr_2HC_n1000B.csv' '3_vars_corr_2HC_n100B.csv'\n",
      " '3_vars_corr_2HC_n300.csv' '3_vars_corr_2HC_n300B.csv'\n",
      " '3_vars_corr_2HC_n500.csv' '3_vars_corr_2HC_n500B.csv'\n",
      " '3_vars_corr_2HC_n700.csv' '3_vars_corr_2HC_n700B.csv'\n",
      " '3_vars_corr_2HC_n900.csv' '3_vars_corr_2HC_n900B.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12880"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulated Datasets\n",
    "dir_path = '/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Model Explanation/ROAD Output'\n",
    "# list to store files name\n",
    "res_ = []\n",
    "res = []\n",
    "for (dir_path, dir_names, file_names) in os.walk(dir_path):\n",
    "        res_.extend(file_names)\n",
    "for file in res_:\n",
    "    if file not in ['GroundTruth.csv', '.Rhistory', '.DS_Store']:\n",
    "        res.append(file)\n",
    "\n",
    "#Extracting the names of the datasets\n",
    "res = [re.sub(r'csv.*', 'csv', i) for i in res]\n",
    "\n",
    "# Eliminating repeated names\n",
    "res = np.unique(res)\n",
    "# Eliminate names that have a parenthesis\n",
    "res = np.array([i for i in res if 'skew' not in i])\n",
    "res = np.array([i for i in res if '30000' not in i])\n",
    "res = np.array([i for i in res if '(' not in i])\n",
    "print(res)\n",
    "\n",
    "#Models of interest\n",
    "model_names = np.array(['RandomForest','LogisticRegression','XGBoost', 'FauxModel'])\n",
    "\n",
    "\n",
    "\n",
    "#Explainers of interest\n",
    "xai_of_int = np.array(['ShapLinear', 'ShapTree', 'ShapPermutation', 'ShapSampling', 'ShapKernelKmeans', 'ShapExact', 'ShapFlow'])\n",
    "\n",
    "#Number of Iterations\n",
    "num_iter = 10\n",
    "iter_list = list(range(num_iter))\n",
    "\n",
    "import itertools\n",
    "#Create a list of all possible combinations of models and explainers\n",
    "comb = list(itertools.product(model_names, xai_of_int, res, iter_list))\n",
    "len(comb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2_vars_corr_1HC_n100.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2_vars_corr_1HC_n100.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2_vars_corr_1HC_n100.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noise</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2_vars_corr_1HC_n100.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1_vars_corr_1HC_n100.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Noise</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0_vars_corr_0HC_n900.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>C1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0_vars_corr_0HC_n1000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>C2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0_vars_corr_0HC_n1000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>C3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0_vars_corr_0HC_n1000.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Noise</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0_vars_corr_0HC_n1000.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Features  Rank                    Dataset\n",
       "0         C1   4.0   2_vars_corr_1HC_n100.csv\n",
       "1         C2   3.0   2_vars_corr_1HC_n100.csv\n",
       "2         C3   2.0   2_vars_corr_1HC_n100.csv\n",
       "3      Noise   1.0   2_vars_corr_1HC_n100.csv\n",
       "4         C1   4.0   1_vars_corr_1HC_n100.csv\n",
       "..       ...   ...                        ...\n",
       "275    Noise   1.0   0_vars_corr_0HC_n900.csv\n",
       "276       C1   4.0  0_vars_corr_0HC_n1000.csv\n",
       "277       C2   3.0  0_vars_corr_0HC_n1000.csv\n",
       "278       C3   2.0  0_vars_corr_0HC_n1000.csv\n",
       "279    Noise   1.0  0_vars_corr_0HC_n1000.csv\n",
       "\n",
       "[280 rows x 3 columns]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Because we are using sinthetic data, we know how each variable is causally affecting the target.\n",
    "# Below, we are going to rank in descending order (larger number means more important) the features.\n",
    "ground = pd.read_csv('/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Synthetic Data/GroundTruth.csv')\n",
    "ground.columns.values[0] = 'Features'\n",
    "ground = ground.melt('Features', var_name='Dataset', value_name='Rank')\n",
    "ground_ = ground.set_index(['Features']).groupby('Dataset').transform(rankdata).reset_index()\n",
    "ground_['Dataset'] = ground['Dataset']\n",
    "ground = ground_\n",
    "\n",
    "# Ensure that for the Skew datasets, the rank is in the correct order\n",
    "# ground.loc[np.logical_and(ground.Dataset.str.contains('skew'), np.logical_not(ground.Features.str.contains('Noise'))), ground.columns[1]] = 3\n",
    "# ground.loc[np.logical_and(ground.Dataset.str.contains('skew'), ground.Features.str.contains('Noise')), ground.columns[1]]= 1\n",
    "\n",
    "\n",
    "ground"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Dimensions of the evaluation files </b> <br> <br>\n",
    "list: 0 = top, 1 = bottom, 2 = random, 3 = shap values <br> <br>\n",
    "Axis 0 = metric : <br>\n",
    "<pre> Metrics: 0 = accu, 1 = accu_balanced, 2 = f1, 3 = auroc, 4 = auprc, 5 = recall </pre>\n",
    "Axis 1 = stage of imputation <br>\n",
    "Axis 2 = repeats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data keep only the Score, Similarity, and ROAD metrics (in case we want to plot later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up results of road\n",
    "path = '/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Model Explanation/ROAD Output'\n",
    "score, score_bottom, score_top, similarity, kendall, random, noise = clean_road_results(model_names, xai_of_int, res, iter_list, ground, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ShapTree', 'ShapPermutation', 'ShapSampling', 'ShapKernelKmeans', 'ShapExact', 'ShapFlow'])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['2_vars_corr_1HC_n1000.csv']['RandomForest'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01132587, 0.01372088, 0.01346247, 0.01335631, 0.0137839 ,\n",
       "       0.01215152, 0.0137839 , 0.01355071, 0.01355071, 0.01373505])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['3_vars_corr_2HC_n100B.csv']['XGBoost']['ShapTree'][3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the score and similarity for every file, model, and explainer\n",
    "score = {}\n",
    "score_bottom = {}\n",
    "score_top = {}\n",
    "similarity = {}\n",
    "kendall = {}\n",
    "random = {}\n",
    "\n",
    "\n",
    "for file in res:\n",
    "\n",
    "    score_ = {}\n",
    "    score_bottom_ = {}\n",
    "    score_top_ = {}\n",
    "    similarity_ = {}\n",
    "    kendall_ = {}\n",
    "    distance_ = {}\n",
    "    random_ = {}\n",
    "    \n",
    "    #Get the ground truth for a particular file\n",
    "    groundnp = ground.loc[ground.Dataset == file].Rank.values\n",
    "    for model in model_names:\n",
    "\n",
    "        _score_ = {}\n",
    "        _score_bottom_ = {}\n",
    "        _score_top_ = {}\n",
    "        _similarity_ = {}\n",
    "        _kendall_ = {}\n",
    "        _random_ = {}\n",
    "        \n",
    "        #Create a dictionary to store the results\n",
    "        for xai in xai_of_int:\n",
    "\n",
    "            for rep in iter_list:\n",
    "                path = f'{dir_path}/{file}_{model}_{xai}_{rep}.joblib'\n",
    "                \n",
    "                if not os.path.isfile(path):\n",
    "                    does_not_exist = True\n",
    "                    # print(f'{path} does not exist')\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                top, bottom, rand, shap_values = joblib.load(path)\n",
    "\n",
    "                if shap_values is None:\n",
    "                    does_not_exist = True\n",
    "                    continue\n",
    "\n",
    "                does_not_exist = False\n",
    "\n",
    "                #stack the results\n",
    "                if rep == 0:\n",
    "                    top_ = top\n",
    "                    bottom_ = bottom\n",
    "                    rand_ = rand\n",
    "                    shap_values_ = shap_values\n",
    "                else:\n",
    "                    top_ = np.dstack([top_, top])\n",
    "                    bottom_ = np.dstack([bottom_, bottom])\n",
    "                    rand_ = np.dstack([rand_, rand])\n",
    "                    shap_values_ = np.dstack([shap_values_, shap_values])\n",
    "    \n",
    "            \n",
    "            if does_not_exist:\n",
    "                continue\n",
    "\n",
    "            evals = [top_, bottom_, rand_, shap_values_]\n",
    "  \n",
    "\n",
    "\n",
    "      \n",
    "                \n",
    "            #Get the score for a particular file, model, and explainer\n",
    "            _score_[xai] = road_score_alt1(evals)\n",
    "            _score_bottom_[xai] = get_score_bottom(evals)\n",
    "            _score_top_[xai] = get_score_top(evals)\n",
    "            _similarity_[xai] = similarity_score(evals)\n",
    "            _kendall_[xai] = get_kendall(evals, groundnp)\n",
    "            _random_[xai] = get_random(evals)\n",
    "            \n",
    "        #  #Plot the results and Save the figure\n",
    "        #     road_plot(evals, model, xai, file)\n",
    "        #     plt.savefig(f'/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/XAI method performacne when Explainaing the PORT Dataset/Figures/Simulated/Full Figure/ROAD/{file}_{model}_{xai}.pdf', dpi=600, bbox_inches = 'tight', transparent = False)\n",
    "        #     plt.close()\n",
    "\n",
    "        if does_not_exist:\n",
    "            continue\n",
    "        \n",
    "        score_[model] = _score_\n",
    "        score_bottom_[model] = _score_bottom_\n",
    "        score_top_[model] = _score_top_\n",
    "        similarity_[model] = _similarity_\n",
    "        kendall_[model] = _kendall_\n",
    "        random_[model] = _random_\n",
    "    \n",
    "    if does_not_exist:\n",
    "        continue\n",
    "\n",
    "    score[file] = score_\n",
    "    score_bottom[file] = score_bottom_\n",
    "    score_top[file] = score_top_\n",
    "    similarity[file] = similarity_\n",
    "    kendall[file] = kendall_\n",
    "    random[file] = random_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned up results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Results/XAI Evaluation Data/noise.joblib']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = '/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Results/XAI Evaluation Data'\n",
    "joblib.dump(score, f'{save_path}/score.joblib', compress=3)\n",
    "joblib.dump(score_bottom, f'{save_path}/score_bottom.joblib', compress=3)\n",
    "joblib.dump(score_top, f'{save_path}/score_top.joblib', compress=3)\n",
    "joblib.dump(similarity, f'{save_path}/similarity.joblib', compress=3)\n",
    "joblib.dump(kendall, f'{save_path}/kendall.joblib', compress=3)\n",
    "joblib.dump(random, f'{save_path}/random.joblib', compress=3)\n",
    "joblib.dump(noise, f'{save_path}/noise.joblib', compress=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract results into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_mean, df_score_bottom_mean, df_score_top_mean, df_sim_std, df_kendall, df_random, df_noise = results_to_dataframes(score, score_bottom, score_top, similarity, kendall, random, noise, metric = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a Data Frame of ROAR Score\n",
    "df_score_mean = {}\n",
    "for file, models in score.items():\n",
    "    df_models = {}\n",
    "    for model, exp in models.items():\n",
    "        df_exp = {}\n",
    "        for explainer, roar_results in exp.items():\n",
    "            df_exp[explainer] = roar_results[3]\n",
    "        df_models[model] = df_exp\n",
    "    df_score_mean[file] = pd.DataFrame(df_models).T\n",
    "df_score_mean = pd.concat(df_score_mean)\n",
    "\n",
    "## Make a Data Frame for ROAR Score Bottom\n",
    "df_score_bottom_mean = {}\n",
    "for file, models in score_bottom.items():\n",
    "    df_models = {}\n",
    "    for model, exp in models.items():\n",
    "        df_exp = {}\n",
    "        for explainer, roar_results in exp.items():\n",
    "            df_exp[explainer] = roar_results[3]\n",
    "        df_models[model] = df_exp\n",
    "    df_score_bottom_mean[file] = pd.DataFrame(df_models).T\n",
    "df_score_bottom_mean = pd.concat(df_score_bottom_mean)\n",
    "\n",
    "## Make a Data Frame for ROAR Score Top\n",
    "df_score_top_mean = {}\n",
    "for file, models in score_top.items():\n",
    "    df_models = {}\n",
    "    for model, exp in models.items():\n",
    "        df_exp = {}\n",
    "        for explainer, roar_results in exp.items():\n",
    "            df_exp[explainer] = roar_results[3]\n",
    "        df_models[model] = df_exp\n",
    "    df_score_top_mean[file] = pd.DataFrame(df_models).T\n",
    "df_score_top_mean = pd.concat(df_score_top_mean)\n",
    "\n",
    "\n",
    "\n",
    "#Data frame of the score\n",
    "df_sim_std = {}\n",
    "for file, models in similarity.items():\n",
    "    df_models = {}\n",
    "    for model, exp in models.items():\n",
    "        df_exp = {}\n",
    "        for explainer, roar_results in exp.items():\n",
    "            df_exp[explainer] = roar_results\n",
    "        df_models[model] = df_exp\n",
    "    df_sim_std[file] = pd.DataFrame(df_models).T\n",
    "df_sim_std = pd.concat(df_sim_std)\n",
    "\n",
    "\n",
    "#Data frame of kendall's tau\n",
    "df = {}\n",
    "for file, models in kendall.items():\n",
    "    df_models = {}\n",
    "    for model, exp in models.items():\n",
    "        df_exp = {}\n",
    "        for explainer, roar_results in exp.items():\n",
    "            df_exp[explainer] = roar_results[0]\n",
    "        df_models[model] = df_exp\n",
    "    df[file] = pd.DataFrame(df_models).T\n",
    "df_kendall = pd.concat(df)\n",
    "\n",
    "\n",
    "# #Data frame of distance\n",
    "# df = {}\n",
    "# for file, models in distance.items():\n",
    "#     df_models = {}\n",
    "#     for model, exp in models.items():\n",
    "#         df_exp = {}\n",
    "#         for explainer, roar_results in exp.items():\n",
    "#             df_exp[explainer] = roar_results[0]\n",
    "#         df_models[model] = df_exp\n",
    "#     df[file] = pd.DataFrame(df_models).T\n",
    "# df_distance = pd.concat(df)\n",
    "\n",
    "\n",
    "#Data frame of random\n",
    "df = {}\n",
    "for file, models in random.items():\n",
    "    df_models = {}\n",
    "    for model, exp in models.items():\n",
    "        df_exp = {}\n",
    "        for explainer, roar_results in exp.items():\n",
    "            df_exp[explainer] = roar_results[3]\n",
    "        df_models[model] = df_exp\n",
    "    df[file] = pd.DataFrame(df_models).T\n",
    "df_random = pd.concat(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join everything into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Repeat</th>\n",
       "      <th>Model</th>\n",
       "      <th>score</th>\n",
       "      <th>score_bottom</th>\n",
       "      <th>score_top</th>\n",
       "      <th>kendall</th>\n",
       "      <th>random</th>\n",
       "      <th>noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_vars_corr_0HC_n100.csv</td>\n",
       "      <td>ShapExact</td>\n",
       "      <td>0</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.272746</td>\n",
       "      <td>-0.009424</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.646414</td>\n",
       "      <td>0.01592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_vars_corr_0HC_n100.csv</td>\n",
       "      <td>ShapExact</td>\n",
       "      <td>1</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.184850</td>\n",
       "      <td>0.099341</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.724540</td>\n",
       "      <td>0.01592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_vars_corr_0HC_n100.csv</td>\n",
       "      <td>ShapExact</td>\n",
       "      <td>2</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.078741</td>\n",
       "      <td>0.203077</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.818854</td>\n",
       "      <td>0.01592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_vars_corr_0HC_n100.csv</td>\n",
       "      <td>ShapExact</td>\n",
       "      <td>3</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.069477</td>\n",
       "      <td>0.211011</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.827088</td>\n",
       "      <td>0.01592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_vars_corr_0HC_n100.csv</td>\n",
       "      <td>ShapExact</td>\n",
       "      <td>4</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.098595</td>\n",
       "      <td>0.185524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.801206</td>\n",
       "      <td>0.01592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10575</th>\n",
       "      <td>3_vars_corr_2HC_n900B.csv</td>\n",
       "      <td>ShapSampling</td>\n",
       "      <td>5</td>\n",
       "      <td>FauxModel</td>\n",
       "      <td>2.625683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.838945</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>3_vars_corr_2HC_n900B.csv</td>\n",
       "      <td>ShapSampling</td>\n",
       "      <td>6</td>\n",
       "      <td>FauxModel</td>\n",
       "      <td>2.625683</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.076989</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.711173</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10577</th>\n",
       "      <td>3_vars_corr_2HC_n900B.csv</td>\n",
       "      <td>ShapSampling</td>\n",
       "      <td>7</td>\n",
       "      <td>FauxModel</td>\n",
       "      <td>2.625683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217564</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.838945</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10578</th>\n",
       "      <td>3_vars_corr_2HC_n900B.csv</td>\n",
       "      <td>ShapSampling</td>\n",
       "      <td>8</td>\n",
       "      <td>FauxModel</td>\n",
       "      <td>2.625683</td>\n",
       "      <td>0.185601</td>\n",
       "      <td>0.039247</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.683236</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10579</th>\n",
       "      <td>3_vars_corr_2HC_n900B.csv</td>\n",
       "      <td>ShapSampling</td>\n",
       "      <td>9</td>\n",
       "      <td>FauxModel</td>\n",
       "      <td>2.625683</td>\n",
       "      <td>0.219420</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.654864</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10580 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Dataset     Explainer  Repeat         Model  \\\n",
       "0       0_vars_corr_0HC_n100.csv     ShapExact       0  RandomForest   \n",
       "1       0_vars_corr_0HC_n100.csv     ShapExact       1  RandomForest   \n",
       "2       0_vars_corr_0HC_n100.csv     ShapExact       2  RandomForest   \n",
       "3       0_vars_corr_0HC_n100.csv     ShapExact       3  RandomForest   \n",
       "4       0_vars_corr_0HC_n100.csv     ShapExact       4  RandomForest   \n",
       "...                          ...           ...     ...           ...   \n",
       "10575  3_vars_corr_2HC_n900B.csv  ShapSampling       5     FauxModel   \n",
       "10576  3_vars_corr_2HC_n900B.csv  ShapSampling       6     FauxModel   \n",
       "10577  3_vars_corr_2HC_n900B.csv  ShapSampling       7     FauxModel   \n",
       "10578  3_vars_corr_2HC_n900B.csv  ShapSampling       8     FauxModel   \n",
       "10579  3_vars_corr_2HC_n900B.csv  ShapSampling       9     FauxModel   \n",
       "\n",
       "          score  score_bottom  score_top   kendall    random    noise  \n",
       "0      2.610254      0.272746  -0.009424  0.333333  0.646414  0.01592  \n",
       "1      2.610254      0.184850   0.099341  0.333333  0.724540  0.01592  \n",
       "2      2.610254      0.078741   0.203077  0.333333  0.818854  0.01592  \n",
       "3      2.610254      0.069477   0.211011  0.333333  0.827088  0.01592  \n",
       "4      2.610254      0.098595   0.185524  0.333333  0.801206  0.01592  \n",
       "...         ...           ...        ...       ...       ...      ...  \n",
       "10575  2.625683      0.000000   0.217564  0.333333  0.838945  0.00000  \n",
       "10576  2.625683      0.152300   0.076989  0.333333  0.711173  0.00000  \n",
       "10577  2.625683      0.000000   0.217564  0.333333  0.838945  0.00000  \n",
       "10578  2.625683      0.185601   0.039247  0.333333  0.683236  0.00000  \n",
       "10579  2.625683      0.219420  -0.002372  0.333333  0.654864  0.00000  \n",
       "\n",
       "[10580 rows x 10 columns]"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tidying(df, value = None):\n",
    "    df = df.reset_index().melt(['level_0', 'level_1', 'level_2'])\n",
    "    df.columns = ['Dataset', 'Explainer', 'Repeat', 'Model', f'{value}']\n",
    "    return df.set_index(['Dataset', 'Explainer', 'Repeat', 'Model'])\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        tidying(df_score_mean, 'score'), \n",
    "        tidying(df_score_bottom_mean, 'score_bottom'), \n",
    "        tidying(df_score_top_mean, 'score_top'), \n",
    "        tidying(df_kendall, 'kendall'), \n",
    "        tidying(df_random, 'random'),\n",
    "        tidying(df_noise, 'noise')\n",
    "    ], \n",
    "    axis=1)\n",
    "df = df.iloc[:, ~df.columns.duplicated()].dropna(axis=0, thresh=5).reset_index()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add known information about the datasets to the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Class Imbalance (Skew) of each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for file in res:\n",
    "    dir_path = f'/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Synthetic Data/Data files with summed target/{file}'\n",
    "    l.append(pd.read_csv(dir_path).Target.mean())\n",
    "    # print(pd.read_csv(dir_path).Target.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the pattern of confounding\n",
    "This is only relevant to distinguish the case where we have 2 Hidden Confounders and all 3 predictive variables being confounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [re.sub(r\".*_n|\\.csv|\\d*|_.*\", '', x) for x in res]\n",
    "f = lambda x: 'A' if x == '' else x\n",
    "a = [f(x) for x in a]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the known information into the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10580, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explainer</th>\n",
       "      <th>Repeat</th>\n",
       "      <th>Model</th>\n",
       "      <th>score</th>\n",
       "      <th>score_bottom</th>\n",
       "      <th>score_top</th>\n",
       "      <th>kendall</th>\n",
       "      <th>random</th>\n",
       "      <th>noise</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Samples</th>\n",
       "      <th>HC</th>\n",
       "      <th>ConfVars</th>\n",
       "      <th>AB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_vars_corr_0HC_n100.csv</th>\n",
       "      <td>ShapExact</td>\n",
       "      <td>0</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.272746</td>\n",
       "      <td>-0.009424</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.646414</td>\n",
       "      <td>0.01592</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_vars_corr_0HC_n100.csv</th>\n",
       "      <td>ShapExact</td>\n",
       "      <td>1</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.184850</td>\n",
       "      <td>0.099341</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.724540</td>\n",
       "      <td>0.01592</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_vars_corr_0HC_n100.csv</th>\n",
       "      <td>ShapExact</td>\n",
       "      <td>2</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.078741</td>\n",
       "      <td>0.203077</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.818854</td>\n",
       "      <td>0.01592</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_vars_corr_0HC_n100.csv</th>\n",
       "      <td>ShapExact</td>\n",
       "      <td>3</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.069477</td>\n",
       "      <td>0.211011</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.827088</td>\n",
       "      <td>0.01592</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_vars_corr_0HC_n100.csv</th>\n",
       "      <td>ShapExact</td>\n",
       "      <td>4</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>2.610254</td>\n",
       "      <td>0.098595</td>\n",
       "      <td>0.185524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.801206</td>\n",
       "      <td>0.01592</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Explainer  Repeat         Model     score  \\\n",
       "Dataset                                                               \n",
       "0_vars_corr_0HC_n100.csv  ShapExact       0  RandomForest  2.610254   \n",
       "0_vars_corr_0HC_n100.csv  ShapExact       1  RandomForest  2.610254   \n",
       "0_vars_corr_0HC_n100.csv  ShapExact       2  RandomForest  2.610254   \n",
       "0_vars_corr_0HC_n100.csv  ShapExact       3  RandomForest  2.610254   \n",
       "0_vars_corr_0HC_n100.csv  ShapExact       4  RandomForest  2.610254   \n",
       "\n",
       "                          score_bottom  score_top   kendall    random  \\\n",
       "Dataset                                                                 \n",
       "0_vars_corr_0HC_n100.csv      0.272746  -0.009424  0.333333  0.646414   \n",
       "0_vars_corr_0HC_n100.csv      0.184850   0.099341  0.333333  0.724540   \n",
       "0_vars_corr_0HC_n100.csv      0.078741   0.203077  0.333333  0.818854   \n",
       "0_vars_corr_0HC_n100.csv      0.069477   0.211011  0.333333  0.827088   \n",
       "0_vars_corr_0HC_n100.csv      0.098595   0.185524  0.333333  0.801206   \n",
       "\n",
       "                            noise  Skew  Samples  HC  ConfVars AB  \n",
       "Dataset                                                            \n",
       "0_vars_corr_0HC_n100.csv  0.01592   0.5      100   0         0  A  \n",
       "0_vars_corr_0HC_n100.csv  0.01592   0.5      100   0         0  A  \n",
       "0_vars_corr_0HC_n100.csv  0.01592   0.5      100   0         0  A  \n",
       "0_vars_corr_0HC_n100.csv  0.01592   0.5      100   0         0  A  \n",
       "0_vars_corr_0HC_n100.csv  0.01592   0.5      100   0         0  A  "
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put in some known information about the datasets\n",
    "df = df.merge(pd.DataFrame(\n",
    "    {'Dataset' : res,\n",
    "    'Skew': l,\n",
    "    'Samples': [np.int64(re.sub(r\".*_n|_skew.*|\\.csv|B\", '', x)) for x in res],\n",
    "    'HC': [np.int64(re.sub(r\".*_corr_|HC.*\", '', x)) for x in res],\n",
    "    'ConfVars': [np.int64(re.sub(r\"_.*\", '', x)) for x in res],\n",
    "    'AB': a}), on='Dataset').set_index('Dataset')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average the results across the iterations\n",
    "df = df.groupby(['Explainer', 'Model', 'Dataset', 'AB']).mean().drop('Repeat', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Results as a single CSV for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/eddie/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Research/Projects/Explainability Method Comparison/Data-ML-XAI-Eval/Results/XAI Evaluation Data/CleanEvals.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('python_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d4e42ab716de065dabed38619282a5a545b37a73470e3b37377cd94259c7c27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
